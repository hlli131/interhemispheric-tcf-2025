{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d171b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import metpy.calc as mpcalc \n",
    "from metpy.units import units \n",
    "from scipy import stats, linalg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.util import add_cyclic_point\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = 'arial'  # times new roman\n",
    "mpl.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e031582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and regrid DAMIP data\n",
    "\n",
    "BASE_PATH = '/mnt/h/Data/CMIP6/DAMIP'\n",
    "OUTPUT_PATH = '/mnt/h/ProcessedData/DAMIP'\n",
    "MODELS = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'CESM2', 'CNRM-CM6-1', 'CanESM5', 'FGOALS-g3', 'GFDL-ESM4',\n",
    "          'GISS-E2-1-G', 'HadGEM3-GC31-LL', 'IPSL-CM6A-LR', 'MIROC6', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NorESM2-LM']\n",
    "EXPERIMENTS = {\n",
    "    'piControl': {'time_slice': None, 'ssp245': False},\n",
    "    'historical': {'time_slice': slice('1980', '2020'), 'ssp245': True},\n",
    "    'hist-GHG': {'time_slice': slice('1980', '2020'), 'ssp245': False},\n",
    "    'hist-aer': {'time_slice': slice('1980', '2020'), 'ssp245': False},\n",
    "    'hist-nat': {'time_slice': slice('1980', '2020'), 'ssp245': False}\n",
    "}\n",
    "VARIABLES = {\n",
    "    'u': {'var_name': 'ua', 'plevs': [85000, 50000, 20000], 'attrs': {'name': 'u component of wind', 'units': 'm/s'}},\n",
    "    'v': {'var_name': 'va', 'plevs': [85000, 20000], 'attrs': {'name': 'v component of wind', 'units': 'm/s'}},\n",
    "    'w': {'var_name': 'wap', 'plevs': [50000], 'attrs': {'name': 'vertical velocity at 500 hPa', 'units': 'Pa/s'}}\n",
    "}\n",
    "GRID_2P5 = {'lat': np.arange(91.25, -91.25, -2.5), 'lon': np.arange(-1.25, 361, 2.5)}\n",
    "\n",
    "\n",
    "def regrid_model_experiment(model, experiment):\n",
    "    print(f\"Regridding {model} {experiment}...\")\n",
    "    exp_config = EXPERIMENTS[experiment]\n",
    " \n",
    "    for var, var_config in VARIABLES.items():\n",
    "        paths = f\"{BASE_PATH}/{model}/{var_config['var_name']}_Amon_{model}_{experiment}_*.nc\"\n",
    "        files = sorted(glob.glob(paths))  \n",
    "        \n",
    "        if experiment == 'historical' and exp_config['ssp245']:\n",
    "            ssp_paths = f\"{BASE_PATH}/{model}/{var_config['var_name']}_Amon_{model}_ssp245_*.nc\"\n",
    "            ssp_files = sorted(glob.glob(ssp_paths))\n",
    "            if xr.open_mfdataset(files, combine='by_coords').time[-1].dt.year <= 2014:\n",
    "                files.append(ssp_files[0]) \n",
    "                ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "            else:\n",
    "                historical_ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "                ssp_ds = xr.open_mfdataset(ssp_files, combine='by_coords')\n",
    "                ssp_ds = ssp_ds.sel(time=slice(str(historical_ds.time[-1].item().year + 1), None))\n",
    "                ds = xr.concat([historical_ds, ssp_ds], dim='time')\n",
    "                historical_ds.close()\n",
    "                ssp_ds.close()\n",
    "        else:       \n",
    "            ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        if exp_config['time_slice'] is not None:\n",
    "            ds = ds.sel(time=exp_config['time_slice'])\n",
    "        \n",
    "        grid_2p5 = xr.Dataset(coords={\n",
    "            'time': ds.time,\n",
    "            'lat': GRID_2P5['lat'],\n",
    "            'lon': GRID_2P5['lon']\n",
    "        })\n",
    "        regridder = xe.Regridder(ds, grid_2p5, 'bilinear')\n",
    "        var_data = regridder(ds[var_config['var_name']])\n",
    "        var_data = var_data.astype(np.float32).isel(lat=slice(1, None), lon=slice(1, None))\n",
    "        var_data = var_data.sel(plev=var_config['plevs'], method='nearest')\n",
    "        var_data.name = var\n",
    "        var_data.attrs.update(var_config['attrs'])\n",
    "        \n",
    "        # save regridded data\n",
    "        output_file = f\"{OUTPUT_PATH}/{model}_{experiment}_{var}_2p5.nc\"\n",
    "        print(f\"Saving {output_file}...\")\n",
    "        var_data.to_netcdf(output_file)\n",
    "        ds.close()\n",
    "        \n",
    "        del ds, regridder, var_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def main():\n",
    "    for model in MODELS:\n",
    "        for experiment in EXPERIMENTS:\n",
    "            regrid_model_experiment(model, experiment)\n",
    "    print(\"Regridding complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DGPI from model simulations\n",
    "\n",
    "def calc_DGPI_from_model_experiment(model, experiment):\n",
    "    u850 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_u_2p5.nc\").u.sel(plev=85000, method='nearest')\n",
    "    u500 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_u_2p5.nc\").u.sel(plev=50000, method='nearest')\n",
    "    u200 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_u_2p5.nc\").u.sel(plev=20000, method='nearest')\n",
    "    v850 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_v_2p5.nc\").v.sel(plev=85000, method='nearest')\n",
    "    v200 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_v_2p5.nc\").v.sel(plev=20000, method='nearest')\n",
    "    w500 = xr.open_dataset(f\"/mnt/h/ProcessedData/DAMIP/{model}_{experiment}_w_2p5.nc\").w.sel(plev=50000, method='nearest')\n",
    "\n",
    "    # calculate VWS magnitude\n",
    "    VWS = np.sqrt((u200 - u850) ** 2 + (v200 - v850) ** 2)     \n",
    "\n",
    "    # calculate dudy500\n",
    "    dudy500 = u500.differentiate(coord='lat', edge_order=1)\n",
    "    dudy500 = dudy500.where(dudy500.lat >= 0, -dudy500)       # reverse the SH\n",
    "    dudy500 /= 111000                                         # convert [m/s/degree] to [1/s]\n",
    "\n",
    "    # calculate AV850\n",
    "    av850 = mpcalc.absolute_vorticity(u850, v850)             # in [1/s]\n",
    "\n",
    "    # calculate DGPI\n",
    "    coef = {\n",
    "        'VWS'    :-1.7,\n",
    "        'dudy500': 2.3,\n",
    "        'w500'   : 3.4,\n",
    "        'av850'  : 2.4\n",
    "    }\n",
    "    VWS_term = (2.0 + 0.1 * VWS) ** (coef['VWS'])\n",
    "    dudy500_term = (5.5 - 1e5 * dudy500) ** (coef['dudy500'])\n",
    "    w500_term = (5.0 - 20 * w500) ** (coef['w500'])\n",
    "    av850_term = (5.5 + np.abs(1e5 * av850.values)) ** (coef['av850'])\n",
    "    DGPI = VWS_term * dudy500_term * w500_term * av850_term * np.exp(-11.8) - 1.0\n",
    "    DGPI = DGPI.drop_vars('plev')\n",
    "\n",
    "    return DGPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf85031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomaly and regional average\n",
    "def calc_annual_from_monthly(data):\n",
    "    annual_data = []\n",
    "    for year in np.unique(data.time.dt.year):\n",
    "        data_year = data.sel(time=data.time.dt.year == year)\n",
    "    \n",
    "        nh_mask = (data_year.lat > 0) & (data_year.time.dt.month.isin([6, 7, 8, 9, 10, 11]))\n",
    "        sh_mask = (data_year.lat < 0) & (data_year.time.dt.month.isin([12, 1, 2, 3, 4, 5]))\n",
    "        \n",
    "        combined_data = xr.concat([\n",
    "            data_year.where(nh_mask, drop=True), \n",
    "            data_year.where(sh_mask, drop=True)\n",
    "        ], dim='lat').sortby('lat', ascending=False)\n",
    "            \n",
    "        yearly_avg = combined_data.sum(dim='time', keep_attrs=True)\n",
    "        yearly_avg = yearly_avg.assign_coords(year=year)\n",
    "        annual_data.append(yearly_avg)\n",
    "    \n",
    "    annual_data = xr.concat(annual_data, dim='year')\n",
    "\n",
    "    return annual_data\n",
    "\n",
    "\n",
    "def calc_anomaly(data, start_year='1981', end_year='2010'):\n",
    "    if data.year.size == 41:\n",
    "        climatology = data.sel(year=slice(start_year, end_year)).mean(dim='year')\n",
    "    else:\n",
    "        climatology = data.mean(dim='year')\n",
    "    data_anomaly = data - climatology\n",
    "    return data_anomaly\n",
    "\n",
    "\n",
    "def calc_region_average(data_anomaly, region='global'):\n",
    "    MDRs = [\n",
    "        {'name': 'WNP', 'lon_min': 120, 'lon_max': 160, 'lat_min': 5, 'lat_max': 25},\n",
    "        {'name': 'ENP', 'lon_min': 240, 'lon_max': 270, 'lat_min': 5, 'lat_max': 20},\n",
    "        {'name': 'NA', 'lon_min': 310, 'lon_max': 345, 'lat_min': 5, 'lat_max': 20},\n",
    "        {'name': 'SI', 'lon_min': 55, 'lon_max': 105, 'lat_min': -20, 'lat_max': -5},\n",
    "        {'name': 'SP', 'lon_min': 150, 'lon_max': 190, 'lat_min': -20, 'lat_max': -5},\n",
    "        {'name': 'NI', 'lon_min': 60, 'lon_max': 95, 'lat_min': 5, 'lat_max': 20}\n",
    "    ]\n",
    "    \n",
    "    if region == 'global':\n",
    "        region_average = []\n",
    "        for r in MDRs:\n",
    "            region_data = data_anomaly.sel(\n",
    "                lon=slice(r['lon_min'], r['lon_max']), \n",
    "                lat=slice(r['lat_max'], r['lat_min'])\n",
    "            ).mean(dim=['lon', 'lat'])\n",
    "            region_average.append(region_data) \n",
    "        region_data = sum(region_average) / len(region_average)\n",
    "        \n",
    "    else:\n",
    "        region_info = next((r for r in MDRs if r['name'] == region), None)\n",
    "        if region_info is None:\n",
    "            raise ValueError('Undefined region')\n",
    "        \n",
    "        region_data = data_anomaly.sel(\n",
    "            lon=slice(region_info['lon_min'], region_info['lon_max']), \n",
    "            lat=slice(region_info['lat_max'], region_info['lat_min'])\n",
    "        ).mean(dim=['lon', 'lat'])\n",
    "    \n",
    "    return region_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98099b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read monthly TCF observations\n",
    "Monthly_TCF_2p5 = xr.open_dataset('/mnt/h/ProcessedData/Monthly_TCF_2p5.nc').tcf\n",
    "TCF_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(Monthly_TCF_2p5)), region='global')\n",
    "\n",
    "# read monthly DGPI observations\n",
    "Monthly_DGPI_2p5 = xr.open_dataset('/mnt/h/ProcessedData/DGPI_2p5.nc').dgpi\n",
    "DGPI_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(Monthly_DGPI_2p5)), region='global')\n",
    "\n",
    "pictrl_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(DGPI_pictrl)), region='global')\n",
    "hist_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(DGPI_hist)), region='global')\n",
    "aer_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(DGPI_aer)), region='global')\n",
    "ghg_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(DGPI_ghg)), region='global')\n",
    "nat_global = calc_region_average(calc_anomaly(calc_annual_from_monthly(DGPI_nat)), region='global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1da92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform optimal fingerprinting using OLS and TLS\n",
    "def optimal_fingerprinting(y, X, pictrl, reg_method, n_boot=1000, ci_range=(2.5, 97.5), random_seed=None):\n",
    "    # input processing\n",
    "    y = y.values if isinstance(y, xr.DataArray) else y\n",
    "    X = X.values if isinstance(X, xr.DataArray) else X\n",
    "    X = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    if reg_method == 'OLS':\n",
    "        beta = LinearRegression(fit_intercept=False).fit(X, y).coef_\n",
    "    \n",
    "    elif reg_method == 'TLS':\n",
    "        Z = np.column_stack([X, y])\n",
    "        _, _, Vh = linalg.svd(Z, full_matrices=False)\n",
    "        V = Vh.T\n",
    "        beta = (-V[:X.shape[1], X.shape[1]:] / V[X.shape[1]:, X.shape[1]:]).flatten()\n",
    "    \n",
    "    results = {'beta': beta}\n",
    "\n",
    "    # bootstrap CIs\n",
    "    pictrl = pictrl.values if isinstance(pictrl, xr.DataArray) else pictrl\n",
    "\n",
    "    boot_beta = np.zeros([n_boot, X.shape[1]])\n",
    "    for i in range(n_boot):\n",
    "        y_boot = y + np.random.choice(pictrl, size=len(y))\n",
    "        if reg_method == 'OLS':\n",
    "            boot_beta[i] = LinearRegression(fit_intercept=False).fit(X, y_boot).coef_\n",
    "        else:\n",
    "            Z_boot = np.column_stack([X, y_boot])\n",
    "            _, _, Vh_boot = linalg.svd(Z_boot, full_matrices=False)\n",
    "            V_boot = Vh_boot.T\n",
    "            boot_beta[i] = (-V_boot[:X.shape[1], X.shape[1]:] / V_boot[X.shape[1]:, X.shape[1]:]).flatten()\n",
    "    \n",
    "    results.update({\n",
    "        'ci_lower': np.percentile(boot_beta, ci_range[0], axis=0),\n",
    "        'ci_upper': np.percentile(boot_beta, ci_range[1], axis=0),\n",
    "        'p_values': np.minimum(2 * np.minimum(\n",
    "            (boot_beta < 0).mean(axis=0),\n",
    "            (boot_beta > 0).mean(axis=0)), 1)\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results, reg_method):\n",
    "    print(f\"\\nOptimal fingerprinting using {reg_method}\")\n",
    "    print(f\"{'forcing':<15} {'beta':>10} {'CI[5-95%]':>20} {'p-value':>10}\")\n",
    "    print('-' * 60)\n",
    "    for i, beta in enumerate(results['beta']):\n",
    "        ci_lower = results.get('ci_lower', [np.nan] * len(results['beta']))[i]\n",
    "        ci_upper = results.get('ci_upper', [np.nan] * len(results['beta']))[i]\n",
    "        p_value = results.get('p_values', [np.nan] * len(results['beta']))[i]\n",
    "        \n",
    "        ci_str = f\"[{ci_lower:.2f}, {ci_upper:.2f}]\" if not np.isnan(ci_lower) else \"N/A\"\n",
    "        p_str = f\"{p_value:.3f}\" if not np.isnan(p_value) else \"N/A\"\n",
    "        print(f\"{f'fingerprint {i+1}':<15} {beta:>10.2f} {ci_str:>20} {p_str:>10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
