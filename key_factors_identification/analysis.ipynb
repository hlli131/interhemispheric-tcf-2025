{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d171b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "from scipy import stats\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import FormatStrFormatter, ScalarFormatter\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from itertools import combinations\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from skexplain import ExplainToolkit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from alibi.explainers import PartialDependence, plot_pd, ALE, plot_ale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = 'arial'  # times new roman\n",
    "mpl.rcParams['font.size'] = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e031582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "annual_TCF = pd.read_csv('/mnt/h/processedData/TimeSeries_TCF.csv')\n",
    "\n",
    "# 5 atmospheric data\n",
    "RH600 = xr.open_dataset('/mnt/h/processedData/rh600_2p5.nc').rh600\n",
    "RV850 = xr.open_dataset('/mnt/h/processedData/rv850_2p5.nc').rv850\n",
    "AV850 = xr.open_dataset('/mnt/h/processedData/av850_2p5.nc').av850\n",
    "VWS = xr.open_dataset('/mnt/h/processedData/VWS_2p5.nc').vws\n",
    "W500 = xr.open_dataset('/mnt/h/processedData/w500_2p5.nc').w500\n",
    "\n",
    "# 6 oceanic data\n",
    "SST = xr.open_dataset('/mnt/h/processedData/SST_2p5.nc').sst\n",
    "SSS = xr.open_dataset('/mnt/h/processedData/SSS_2p5.nc').sss\n",
    "MLD = xr.open_dataset('/mnt/h/processedData/MLDp03_2p5.nc').mldp03\n",
    "D26 = xr.open_dataset('/mnt/h/processedData/D26_2p5.nc').d26\n",
    "T100 = xr.open_dataset('/mnt/h/processedData/T100_2p5.nc').t100\n",
    "TCHP = xr.open_dataset('/mnt/h/processedData/TCHP_2p5.nc').tchp\n",
    "\n",
    "# MPI\n",
    "MPI = xr.open_dataset('/mnt/h/processedData/MPI_2p5.nc').mpi\n",
    "\n",
    "# define MDRs\n",
    "MDRs = {\n",
    "    'WNP': {'lon_min': 120, 'lon_max': 160, 'lat_min': 5, 'lat_max': 25},\n",
    "    'ENP': {'lon_min': 240, 'lon_max': 270, 'lat_min': 5, 'lat_max': 20},\n",
    "    'NA': {'lon_min': 310, 'lon_max': 345, 'lat_min': 5, 'lat_max': 20},\n",
    "    'SI': {'lon_min': 55, 'lon_max': 105, 'lat_min': -15, 'lat_max': -5},\n",
    "    'SP': {'lon_min': 150, 'lon_max': 190, 'lat_min': -20, 'lat_max': -5},\n",
    "    'NI': {'lon_min': 60, 'lon_max': 95, 'lat_min': 5, 'lat_max': 20}\n",
    "}\n",
    "\n",
    "# select data in MDRs (6–11 for NH and 12–5 for SH)\n",
    "def sel_mdr_data(dataset, mdr_name):\n",
    "    mdr = MDRs[mdr_name]  \n",
    "    if mdr['lat_min'] > 0:\n",
    "        data_mdr = dataset.sel(\n",
    "            lon=slice(mdr['lon_min'], mdr['lon_max']),\n",
    "            lat=slice(mdr['lat_max'], mdr['lat_min']),\n",
    "        )\n",
    "        data_mdr = data_mdr.where(data_mdr['time.month'].isin([6, 7, 8, 9, 10, 11]), drop=True)\n",
    "    else: \n",
    "        data_mdr = dataset.sel(\n",
    "            lon=slice(mdr['lon_min'], mdr['lon_max']),\n",
    "            lat=slice(mdr['lat_max'], mdr['lat_min']),\n",
    "        )\n",
    "        data_mdr = data_mdr.where(data_mdr['time.month'].isin([12, 1, 2, 3, 4, 5]), drop=True)\n",
    "    \n",
    "    return data_mdr\n",
    "\n",
    "variables = {\n",
    "    'RH600': RH600,\n",
    "    'RV850': RV850,\n",
    "    'AV850': AV850,\n",
    "    'VWS': VWS,\n",
    "    'W500': W500,\n",
    "    'SST': SST,\n",
    "    'SSS': SSS,\n",
    "    'MLD': MLD,\n",
    "    'D26': D26,\n",
    "    'T100': T100,\n",
    "    'TCHP': TCHP,\n",
    "    'MPI': MPI,\n",
    "}\n",
    "\n",
    "data_selected = {mdr: {} for mdr in MDRs.keys()}\n",
    "for var_name, var_data in variables.items():\n",
    "    for mdr_name in MDRs.keys():\n",
    "        data_selected[mdr_name][var_name] = sel_mdr_data(var_data, mdr_name)\n",
    "\n",
    "data_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data merging\n",
    "merged_data = annual_TCF.copy()\n",
    "merged_data = merged_data.rename(columns={'SEASON': 'year'})\n",
    "\n",
    "for mdr_name in data_selected.keys():\n",
    "    for var_name in data_selected[mdr_name].keys():   \n",
    "        ds = data_selected[mdr_name][var_name]\n",
    "        if 'depth' in ds.dims:\n",
    "            annual_mean = ds.mean(dim=['lat', 'lon', 'depth']).groupby('time.year').mean()\n",
    "        else:\n",
    "            annual_mean = ds.mean(dim=['lat', 'lon']).groupby('time.year').mean()\n",
    "        annual_mean_df = annual_mean.to_dataframe(name=f'{mdr_name}_{var_name}').reset_index()\n",
    "    \n",
    "        merged_data = merged_data.merge(\n",
    "            annual_mean_df, \n",
    "            left_on='year', \n",
    "            right_on='year', \n",
    "            how='left',\n",
    "        )\n",
    "\n",
    "for var in variables:\n",
    "    cols = [f'{basin}_{var}' for basin in ['WNP', 'ENP', 'NA', 'SI', 'SP', 'NI']]\n",
    "    \n",
    "    # reverse the sign of AV850 and RV850 in SH\n",
    "    if var in ['AV850', 'RV850']:    \n",
    "        weights = [1, 1, 1, -1, -1, 1]\n",
    "        weighted_data = merged_data[cols] * weights\n",
    "        merged_data[f'ALL_{var}'] = weighted_data.mean(axis=1)\n",
    "    else:\n",
    "        merged_data[f'ALL_{var}'] = merged_data[cols].mean(axis=1)\n",
    "\n",
    "merged_data\n",
    "\n",
    "# save data\n",
    "# merged_data.to_csv('H:/ProcessedData/IML_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78501723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation function\n",
    "def evaluate_model(model, X, y, name='Test'):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = root_mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y, y_pred) * 100\n",
    "    \n",
    "    print(f'{name} set metrics:')\n",
    "    print(f'R2: {r2:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'MAPE: {mape:.4f}%')\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.scatter(y, y_pred, alpha=0.6)\n",
    "    plt.plot([60, 100], [60, 100], 'k--', lw=2)\n",
    "    plt.xlim([60, 100])\n",
    "    plt.xticks([60, 80, 100])\n",
    "    plt.ylim([60, 100])\n",
    "    plt.yticks([60, 80, 100])\n",
    "    plt.xlabel('Observations')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'{name} set')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add54602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for global-scale analysis\n",
    "X = merged_data.filter(regex='ALL_')\n",
    "y = merged_data['All']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1577)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=50,\n",
    "    criterion='friedman_mse', \n",
    "    max_depth=5, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features=1.0, \n",
    "    bootstrap=True, \n",
    "    n_jobs=-1, \n",
    "    random_state=1983, \n",
    "    max_samples=1.0, \n",
    ")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': range(20, 26),\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "}\n",
    "rf_gs = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=rf_params,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='r2',\n",
    ")\n",
    "rf_gs.fit(X_train, y_train)\n",
    "best_rf_params = rf_gs.best_params_\n",
    "best_rf = rf_gs.best_estimator_\n",
    "\n",
    "evaluate_model(best_rf, X_train, y_train, name='Training')\n",
    "evaluate_model(best_rf, X_test, y_test, name='Test')\n",
    "\n",
    "pd.Series(best_rf.feature_importances_, index=X.columns).sort_values().tail(12).plot(kind='barh')\n",
    "plt.title('Random forest feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164de279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost for global-scale analysis\n",
    "X = merged_data.filter(regex='ALL_')\n",
    "y = merged_data['All']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1577)\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1,\n",
    "    min_child_weight=1.0,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5,\n",
    "    random_state=1577,  \n",
    ")\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': range(5, 15),\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.3, 0.4, 0.5],\n",
    "    'reg_alpha': [0.5, 1.0, 1.5],\n",
    "    'reg_lambda': [0.4, 0.6, 0.8],  \n",
    "}\n",
    "xgb_gs = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_params,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='r2',\n",
    ")\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "best_xgb_params = xgb_gs.best_params_\n",
    "best_xgb = xgb_gs.best_estimator_\n",
    "\n",
    "evaluate_model(best_xgb, X_train, y_train, name='Training')\n",
    "evaluate_model(best_xgb, X_test, y_test, name='Test')\n",
    "\n",
    "pd.Series(best_xgb.feature_importances_, index=X.columns).sort_values().tail(12).plot(kind='barh')\n",
    "plt.title('XGBoost feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce73418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM for global-scale analysis\n",
    "X = merged_data.filter(regex='ALL_')\n",
    "y = merged_data['All']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1577)\n",
    "\n",
    "lgb = LGBMRegressor(\n",
    "    num_leaves=5,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,  \n",
    "    n_estimators=50,\n",
    "    min_child_weight=1.0,\n",
    "    min_child_samples=4,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.8,\n",
    "    reg_lambda=0.8,\n",
    "    random_state=1577,  \n",
    "    n_jobs=-1,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': [5],\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [20],\n",
    "    'reg_alpha': [0.8,],\n",
    "    'reg_lambda': [1.2],  \n",
    "}\n",
    "lgb_gs = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=lgb_params,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='r2',\n",
    ")\n",
    "lgb_gs.fit(X_train, y_train)\n",
    "best_lgb_params = lgb_gs.best_params_\n",
    "best_lgb = lgb_gs.best_estimator_\n",
    "\n",
    "evaluate_model(best_lgb, X_train, y_train, name='Training')\n",
    "evaluate_model(best_lgb, X_test, y_test, name='Test')\n",
    "\n",
    "pd.Series(best_lgb.feature_importances_, index=X.columns).sort_values().tail(12).plot(kind='barh')\n",
    "plt.title('LightGBM feature importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
